#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¹ all_channels_export_*.txtï¼ˆå¾ˆå¤§ï¼‰æ–‡ä»¶åšå¿«é€ŸæŠ½æ ·æ¸…æ´—ï¼š
- æ¯éš” N è¡ŒæŠ½å–ä¸€è¡Œï¼Œå½¢æˆæ ·æœ¬
- æ¸…æ´—è§„åˆ™ï¼š
  1) ä»…å°† https://t.me/ é“¾æ¥ç”¨äºè¯†åˆ« channel å­—æ®µï¼Œå¹¶ä»æ ‡é¢˜/æè¿°/æ ‡ç­¾ä¸­ç§»é™¤
  2) ç§»é™¤å™ªå£°å…³é”®è¯ï¼š"é¢‘é“"ã€"æœç´¢ç»“æœ"ã€"å¤¸å…‹é¢‘é“"ã€"ç¾¤ç»„"ã€"æŠ•ç¨¿/æœç´¢"ã€"æ¥è‡ªï¼š[é›·é”‹]"ã€"æŠ•ç¨¿"
  3) ç§»é™¤ @usernameï¼ˆåŒ…æ‹¬ @yunpans ç­‰ï¼‰åœ¨ title/description/tags ä¸­çš„å‡ºç°
  4) è§£æç½‘ç›˜é“¾æ¥ï¼ˆå¤¸å…‹/é˜¿é‡Œ/115/ç™¾åº¦/å¤©ç¿¼/UC/è¿…é›·/123pan/123684/ç§»åŠ¨äº‘ï¼‰
- äº§å‡ºï¼šcleaned_sample.jsonlï¼ˆJSONLï¼‰
- å¯é€‰ï¼š--import å¯¼å…¥å‰ 10 æ¡æ¸…æ´—æ•°æ®ä»¥æµ‹è¯•
"""

import argparse
import json
import os
import re
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional, Tuple

from sqlalchemy.orm import Session

from model import Message, engine
from import_historical_data import extract_links_from_text, extract_tags_from_text

BEIJING_TZ = timezone(timedelta(hours=8))

NOISE_KEYWORDS = [
    'é¢‘é“', 'æœç´¢ç»“æœ', 'å¤¸å…‹é¢‘é“', 'ç¾¤ç»„', 'æŠ•ç¨¿/æœç´¢', 'æ¥è‡ªï¼š[é›·é”‹]', 'æŠ•ç¨¿'
]

TELEGRAM_LINK_RE = re.compile(r'https?://t\.me/([A-Za-z0-9_+]+)/?')
AT_HANDLE_RE = re.compile(r'@([A-Za-z0-9_]{3,})')

# ç®€å• URL æå–ï¼Œç”¨äºåœ¨æ¸…æ´—åå‰”é™¤ t.me URL
URL_RE = re.compile(r'https?://[^\s]+')


def to_naive_beijing(dt: datetime) -> datetime:
    if dt.tzinfo is None:
        return dt
    return dt.astimezone(BEIJING_TZ).replace(tzinfo=None)


def parse_timestamp(raw: Optional[str]) -> datetime:
    if not raw:
        return to_naive_beijing(datetime.now(BEIJING_TZ))
    try:
        # å…¼å®¹ "2025-09-04 15:17:15.849487" ä¸ ISO8601
        raw2 = raw.replace('Z', '+00:00')
        # å¦‚æœæ²¡æœ‰æ—¶åŒºï¼Œå½“ä½œåŒ—äº¬æ—¶é—´
        dt = datetime.fromisoformat(raw2)
        if dt.tzinfo is None:
            return to_naive_beijing(dt.replace(tzinfo=BEIJING_TZ))
        return to_naive_beijing(dt)
    except Exception:
        try:
            # å¸¸è§æ ¼å¼å…œåº•
            return to_naive_beijing(datetime.strptime(raw, '%Y-%m-%d %H:%M:%S'))
        except Exception:
            return to_naive_beijing(datetime.now(BEIJING_TZ))


def remove_noise(text: str) -> str:
    if not text:
        return ''
    out = text
    for kw in NOISE_KEYWORDS:
        out = out.replace(kw, '')
    return out


def strip_at_handles(text: str) -> str:
    # åˆ é™¤ @username ä»¥åŠç´§éšçš„ç©ºç™½
    return AT_HANDLE_RE.sub('', text)


def extract_channel_from_text(text: str) -> Optional[str]:
    # ä» t.me é“¾æ¥ä¸­æå– channel åç§°ï¼ˆç¬¬ä¸€æ®µ pathï¼‰
    # ä»…æ¥å—ç”±å­—æ¯/æ•°å­—/ä¸‹åˆ’çº¿ç»„æˆçš„ç”¨æˆ·åï¼ˆå¿½ç•¥ + é‚€è¯·ï¼‰
    matches = TELEGRAM_LINK_RE.findall(text or '')
    for m in matches:
        if m and not m.startswith('+') and re.fullmatch(r'[A-Za-z0-9_]{3,}', m):
            return m
    return None


def normalize_channel(src_channel: Optional[str], detected_username: Optional[str]) -> str:
    """å°† channel è§„èŒƒä¸º https://t.me/<username>ï¼Œä¼˜å…ˆä½¿ç”¨ src_channel ä¸­çš„ t.me æˆ– @handleï¼Œå…¶æ¬¡ä½¿ç”¨ detected_username"""
    def to_url(u: str) -> Optional[str]:
        if not u:
            return None
        u = u.strip()
        if u.startswith('http://t.me/') or u.startswith('https://t.me/'):
            # æå–ç”¨æˆ·åéƒ¨åˆ†
            m = re.search(r'https?://t\.me/([A-Za-z0-9_]+)', u)
            if m:
                return f"https://t.me/{m.group(1)}"
            return None
        if u.startswith('@'):
            u = u[1:]
        if re.fullmatch(r'[A-Za-z0-9_]{3,}', u):
            return f"https://t.me/{u}"
        return None

    # 1) å°è¯• src_channel
    url = to_url(src_channel or '')
    if url:
        return url
    # 2) å°è¯• detected_username
    url = to_url(detected_username or '')
    if url:
        return url
    return ''


def remove_telegram_links(text: str) -> str:
    # ç§»é™¤æ‰€æœ‰ t.me é“¾æ¥
    def repl(m):
        url = m.group(0)
        if url.startswith('https://t.me/') or url.startswith('http://t.me/'):
            return ''
        return url
    return URL_RE.sub(repl, text)


def text_to_title_desc(text: str) -> (str, str):
    lines = [ln.strip() for ln in (text or '').split('\n')]
    lines = [ln for ln in lines if ln]
    if not lines:
        return '', ''
    title = lines[0][:200]
    desc = '\n'.join(lines[1:]).strip()
    return title, desc


def clean_record(raw_line: str) -> Optional[Dict[str, Any]]:
    raw_line = raw_line.strip()
    if not raw_line:
        return None

    # å¦‚æœæ•´è¡ŒåŒ…å«å™ªå£°å…³é”®è¯ï¼Œç›´æ¥ä¸¢å¼ƒè¯¥è®°å½•
    lowered = raw_line
    for kw in NOISE_KEYWORDS:
        if kw in lowered:
            return None

    src_channel = None
    src_timestamp = None
    src_title = None
    src_desc = None
    src_tags: List[str] = []
    src_text = None

    # 1) å…ˆå°è¯• JSON è§£æ
    data = None
    try:
        data = json.loads(raw_line)
    except Exception:
        data = None

    if isinstance(data, dict):
        # å…¼å®¹å‡ ç§å¸¸è§å­—æ®µ
        src_text = data.get('text') or data.get('message') or ''
        src_title = data.get('title')
        src_desc = data.get('description')
        src_tags = data.get('tags') or []
        src_timestamp = data.get('timestamp') or data.get('date') or data.get('created_at')
        src_channel = data.get('channel') or data.get('chat') or None

        # å¦‚æœæ²¡æœ‰ textï¼Œå°† title/description æ‹¼ä¸º text ä»¥ä¾¿ç»Ÿä¸€æ¸…æ´—
        if not src_text:
            parts = []
            if src_title:
                parts.append(str(src_title))
            if src_desc:
                parts.append(str(src_desc))
            src_text = '\n'.join(parts)
    else:
        # é JSONï¼ŒæŒ‰çº¯æ–‡æœ¬å¤„ç†
        src_text = raw_line

    # 2) æŠ½å– channelï¼ˆæ¥è‡ª t.meï¼‰ï¼Œå¹¶ä»æ–‡æœ¬ä¸­ç§»é™¤ t.me é“¾æ¥
    detected_channel = extract_channel_from_text(src_text or '')
    # è§„èŒƒä¸º URLï¼ˆhttps://t.me/<username>ï¼‰ï¼Œæ»¡è¶³â€œè¿™ç§åªèƒ½æ”¾åœ¨ channel å­—æ®µâ€
    channel_url = normalize_channel(src_channel, detected_channel)

    # 3) åˆæˆåŸå§‹æ–‡æœ¬ç”¨äºæ¸…æ´—
    combined_text = src_text or ''

    # 4) ç§»é™¤å™ªå£°å…³é”®è¯
    combined_text = remove_noise(combined_text)

    # 5) ä»æ–‡æœ¬ä¸­ç§»é™¤ t.me é“¾æ¥
    combined_text = remove_telegram_links(combined_text)

    # 6) ç§»é™¤ @handles
    combined_text = strip_at_handles(combined_text)

    # 7) è§£æç½‘ç›˜é“¾æ¥
    links = extract_links_from_text(combined_text)

    if not links:
        # æ²¡æœ‰ç½‘ç›˜é“¾æ¥ï¼Œè·³è¿‡
        return None

    # 8) è§£ææ ‡ç­¾ï¼šä¼˜å…ˆ JSON tagsï¼Œå¦åˆ™ä»æ¸…æ´—åçš„æ–‡æœ¬ä¸­æå–
    tags = list({*(src_tags or []), *extract_tags_from_text(combined_text)}) if combined_text else (src_tags or [])
    # å†æ¬¡æ¸…ç† tags ä¸­çš„ @xxx
    tags = [AT_HANDLE_RE.sub('', t).strip() for t in tags if t and t.strip()]
    tags = [t for t in tags if t]

    # 9) æ„é€ æ ‡é¢˜ä¸æè¿°
    title, description = text_to_title_desc(combined_text)

    # å¦‚æœ JSON è‡ªå¸¦æ ‡é¢˜ï¼Œä¸”æ¸…æ´—åçš„æ ‡é¢˜ä¸ºç©ºï¼Œå¯å›é€€
    if not title and src_title:
        title = strip_at_handles(remove_noise(str(src_title)))[:200]
    if not description and src_desc:
        description = strip_at_handles(remove_noise(str(src_desc)))

    # 10) æ—¶é—´
    timestamp = parse_timestamp(src_timestamp)

    cleaned = {
        'timestamp': timestamp.strftime('%Y-%m-%d %H:%M:%S'),
        'title': title,
        'description': description,
        'links': links,
        'tags': tags,
        'source': 'cleaned_export',
        'channel': channel_url or '',
        'group_name': '',
        'bot': '',
        'created_at': datetime.now(BEIJING_TZ).strftime('%Y-%m-%d %H:%M:%S')
    }
    return cleaned


def sample_and_clean(inputs: List[str], interval: int, limit_preview: int = 100) -> List[Dict[str, Any]]:
    """å¯¹å¤šä¸ªå¤§æ–‡ä»¶è¿›è¡ŒæŠ½æ ·ï¼ˆæ¯ interval è¡ŒæŠ½ 1 è¡Œï¼‰ï¼Œå¹¶æ¸…æ´—ï¼Œè¿”å›æ ·æœ¬åˆ—è¡¨ã€‚
    ç­–ç•¥ï¼šæŒ‰ offset ä» 0..interval-1 é€æ­¥æŠ½æ ·ï¼ˆå…ˆ 0 å† 1...ï¼‰ï¼Œå°½é‡å‡‘å¤Ÿ limit_previewã€‚
    """
    results: List[Dict[str, Any]] = []
    if interval <= 0:
        interval = 1000

    # å…ˆå°è¯• offset=0ï¼ˆç»å…¸æ¯1000è¡Œç¬¬1000è¡Œï¼‰ï¼Œä¸è¶³å†é€æ­¥åç§»
    for offset in range(0, interval):
        if len(results) >= limit_preview:
            break
        for path in inputs:
            if len(results) >= limit_preview:
                break
            if not os.path.exists(path):
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                continue
            picked_in_this_file = 0
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                for idx, line in enumerate(f, 1):
                    if len(results) >= limit_preview:
                        break
                    # idx % interval == 0 å¯¹åº” offset=0ï¼›é€šç”¨ä¸º (idx - offset) % interval == 0
                    if (idx - offset) % interval != 0:
                        continue
                    cleaned = clean_record(line)
                    if cleaned:
                        results.append(cleaned)
                        picked_in_this_file += 1
            if picked_in_this_file:
                print(f"ğŸ“¦ {os.path.basename(path)} @offset={offset} æŠ½åˆ° {picked_in_this_file} æ¡")
    return results


def write_jsonl(path: str, rows: List[Dict[str, Any]]):
    with open(path, 'w', encoding='utf-8') as f:
        for row in rows:
            f.write(json.dumps(row, ensure_ascii=False) + '\n')
    print(f"âœ… å·²å†™å…¥æ ·æœ¬æ–‡ä»¶: {path} ï¼ˆå…± {len(rows)} æ¡ï¼‰")


def import_first_n(rows: List[Dict[str, Any]], n: int = 10) -> List[int]:
    """å°†æ ·æœ¬å¯¼å…¥æ•°æ®åº“ï¼ˆMessage è¡¨ï¼‰ï¼ŒæŒ‰æˆåŠŸè®¡æ•°ç›´åˆ°æ’å…¥æ»¡ n æ¡æˆ–æ ·æœ¬ç”¨å°½ï¼›è¿”å›æ’å…¥çš„è®°å½• id åˆ—è¡¨"""
    inserted = 0
    inserted_ids: List[int] = []
    with Session(engine) as session:
        for row in rows:
            if inserted >= n:
                break
            try:
                ts = parse_timestamp(row.get('timestamp'))
                ca = parse_timestamp(row.get('created_at'))

                # å»é‡ï¼šä»»ä¸€ç½‘ç›˜é“¾æ¥å­˜åœ¨åˆ™è·³è¿‡è¯¥æ¡
                exists = None
                for netdisk, link in (row.get('links') or {}).items():
                    q = session.query(Message).filter(
                        Message.links.op('->>')(netdisk) == link
                    ).first()
                    if q:
                        exists = q
                        break
                if exists:
                    continue

                msg = Message(
                    timestamp=ts,
                    title=row.get('title') or '',
                    description=row.get('description') or '',
                    links=row.get('links') or {},
                    tags=row.get('tags') or [],
                    source=row.get('source') or 'cleaned_export',
                    channel=row.get('channel') or '',
                    group_name=row.get('group_name') or '',
                    bot=row.get('bot') or '',
                    created_at=ca,
                )
                session.add(msg)
                session.flush()  # å…ˆè·å–è‡ªå¢ ID
                inserted_ids.append(msg.id)
                inserted += 1
            except Exception as e:
                print(f"âŒ æ’å…¥å¤±è´¥: {e}")
        session.commit()
    print(f"âœ… å·²å¯¼å…¥ {inserted} æ¡æ ·æœ¬è®°å½•åˆ°æ•°æ®åº“")
    return inserted_ids


def query_and_print_imported(n: int, run_start_naive: datetime):
    """æŸ¥è¯¢å¹¶æ‰“å°æœ¬æ¬¡è¿è¡Œæ’å…¥çš„æœ€è¿‘ n æ¡è®°å½•ï¼ˆä¾æ® created_at >= run_start_naive ä¸” source=cleaned_exportï¼‰"""
    with Session(engine) as session:
        rows = (
            session.query(Message)
            .filter(Message.source == 'cleaned_export')
            .filter(Message.created_at >= run_start_naive)
            .order_by(Message.created_at.desc())
            .limit(n)
            .all()
        )
        print(f"\n===== æœ¬æ¬¡å¯¼å…¥çš„ {len(rows)} æ¡è®°å½•ï¼ˆå€’åºï¼‰=====\n")
        for m in rows:
            obj = {
                'id': getattr(m, 'id', None),
                'timestamp': m.timestamp.strftime('%Y-%m-%d %H:%M:%S') if m.timestamp else None,
                'title': m.title,
                'description': m.description,
                'links': m.links,
                'tags': m.tags,
                'source': m.source,
                'channel': m.channel,
                'group_name': m.group_name,
                'bot': m.bot,
                'created_at': m.created_at.strftime('%Y-%m-%d %H:%M:%S') if m.created_at else None,
            }
            print(json.dumps(obj, ensure_ascii=False))


def query_and_print_by_ids(ids: List[int]):
    if not ids:
        print("âš ï¸ æ— å¯æŸ¥è¯¢çš„ ID")
        return
    with Session(engine) as session:
        rows = (
            session.query(Message)
            .filter(Message.id.in_(ids))
            .order_by(Message.id.asc())
            .all()
        )
        print(f"\n===== æŒ‰IDå›æ˜¾æœ¬æ¬¡å¯¼å…¥çš„ {len(rows)} æ¡è®°å½• =====\n")
        for m in rows:
            obj = {
                'id': getattr(m, 'id', None),
                'timestamp': m.timestamp.strftime('%Y-%m-%d %H:%M:%S') if m.timestamp else None,
                'title': m.title,
                'description': m.description,
                'links': m.links,
                'tags': m.tags,
                'source': m.source,
                'channel': m.channel,
                'group_name': m.group_name,
                'bot': m.bot,
                'created_at': m.created_at.strftime('%Y-%m-%d %H:%M:%S') if m.created_at else None,
            }
            print(json.dumps(obj, ensure_ascii=False))


def find_existing_by_links(session: Session, links: Dict[str, str]) -> Optional[Message]:
    if not links:
        return None
    # é€ä¸ªç½‘ç›˜é”®æŸ¥è¯¢ï¼Œä»»ä¸€åŒ¹é…å³è®¤ä¸ºå­˜åœ¨
    for netdisk, link in links.items():
        if not link:
            continue
        try:
            m = (
                session.query(Message)
                .filter(Message.links.op('->>')(netdisk) == link)
                .first()
            )
            if m:
                return m
        except Exception:
            # æŸäº›æ•°æ®åº“é€‚é…å™¨å¯èƒ½ä¸æ”¯æŒ ->> æ“ä½œï¼Œå¿½ç•¥å¼‚å¸¸ç»§ç»­
            continue
    return None


def upsert_row(session: Session, row: Dict[str, Any]) -> Tuple[str, int]:
    """æŒ‰é“¾æ¥å»é‡å¹¶è¦†ç›–ï¼š
    - è‹¥å­˜åœ¨ç›¸åŒé“¾æ¥çš„è®°å½•ï¼šè¦†ç›–æ›´æ–°å¹¶è¿”å› ('updated', id)
    - å¦åˆ™æ’å…¥æ–°è®°å½•å¹¶è¿”å› ('inserted', id)
    """
    ts = parse_timestamp(row.get('timestamp'))
    ca = parse_timestamp(row.get('created_at'))

    links = row.get('links') or {}
    existing = find_existing_by_links(session, links)
    if existing:
        # è¦†ç›–å­—æ®µ
        existing.timestamp = ts
        existing.title = row.get('title') or ''
        existing.description = row.get('description') or ''
        existing.links = links
        existing.tags = row.get('tags') or []
        existing.source = row.get('source') or 'cleaned_export'
        existing.channel = row.get('channel') or ''
        existing.group_name = row.get('group_name') or ''
        existing.bot = row.get('bot') or ''
        existing.created_at = ca
        session.add(existing)
        session.flush()
        return ('updated', int(existing.id or 0))

    # æ’å…¥
    msg = Message(
        timestamp=ts,
        title=row.get('title') or '',
        description=row.get('description') or '',
        links=links,
        tags=row.get('tags') or [],
        source=row.get('source') or 'cleaned_export',
        channel=row.get('channel') or '',
        group_name=row.get('group_name') or '',
        bot=row.get('bot') or '',
        created_at=ca,
    )
    session.add(msg)
    session.flush()
    return ('inserted', int(msg.id or 0))


def process_full_clean_only(inputs: List[str], output_path: str, dedup: bool = True) -> Dict[str, int]:
    """å…¨é‡æ¸…æ´—ï¼ˆä¸å…¥åº“ï¼‰ï¼Œå°†æ¸…æ´—ç»“æœå†™å…¥ JSONLï¼›åœ¨æ¸…æ´—é˜¶æ®µæŒ‰â€œé“¾æ¥ç›¸åŒå³é‡å¤â€å»é‡ã€‚
    è¿”å›ç»Ÿè®¡ï¼š{'processed': n, 'written': w, 'skipped': s, 'dedup_skipped': d}
    """
    if not output_path:
        raise ValueError('output_path ä¸èƒ½ä¸ºç©º')
    processed = written = skipped = dedup_skipped = 0
    seen_links = set()  # ä»¥é“¾æ¥å­—ç¬¦ä¸²å»é‡
    with open(output_path, 'w', encoding='utf-8') as fout:
        for path in inputs:
            if not os.path.exists(path):
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                continue
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    processed += 1
                    cleaned = clean_record(line)
                    if not cleaned:
                        skipped += 1
                        continue
                    if dedup:
                        links = cleaned.get('links') or {}
                        # å¦‚æœä»»ä¸€é“¾æ¥å·²å‡ºç°ï¼Œåˆ™è·³è¿‡æœ¬æ¡
                        is_dup = False
                        for _k, _v in links.items():
                            if not _v:
                                continue
                            key = _v.strip()
                            if key in seen_links:
                                is_dup = True
                                break
                        if is_dup:
                            dedup_skipped += 1
                            continue
                        # å°†æœ¬æ¡çš„æ‰€æœ‰é“¾æ¥åŠ å…¥å·²è§é›†åˆ
                        for _k, _v in links.items():
                            if _v:
                                seen_links.add(_v.strip())
                    fout.write(json.dumps(cleaned, ensure_ascii=False) + '\n')
                    written += 1
    print(f"\n===== å…¨é‡æ¸…æ´—å®Œæˆï¼ˆä»…è¾“å‡ºJSONLï¼‰ =====")
    print(f"å¤„ç†è¡Œæ•°: {processed}")
    print(f"å†™å…¥: {written} æ¡")
    print(f"è·³è¿‡(æ— é“¾æ¥/å™ªå£°/é”™è¯¯): {skipped} æ¡")
    if dedup:
        print(f"å»é‡è·³è¿‡: {dedup_skipped} æ¡")
    return {'processed': processed, 'written': written, 'skipped': skipped, 'dedup_skipped': dedup_skipped}


def process_full_upsert(inputs: List[str], output_path: str = '', commit_every: int = 500) -> Dict[str, int]:
    """å…¨é‡æ¸…æ´—å¹¶è¦†ç›–å¯¼å…¥æ•°æ®åº“ï¼š
    - é€è¡Œè¯»å– inputs ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    - æ¸…æ´—åè‹¥å­˜åœ¨é“¾æ¥åˆ™ upsert åˆ°æ•°æ®åº“ï¼ˆæŒ‰é“¾æ¥ç›¸åŒå³é‡å¤ï¼‰
    - å¯é€‰å°†æ¸…æ´—åçš„æ¯æ¡å†™å…¥ output_pathï¼ˆJSONLï¼‰
    è¿”å›ç»Ÿè®¡ä¿¡æ¯ï¼š{'inserted': x, 'updated': y, 'skipped': z, 'processed': n}
    """
    inserted = updated = skipped = processed = 0
    fout = None
    if output_path:
        fout = open(output_path, 'w', encoding='utf-8')

    with Session(engine) as session:
        for path in inputs:
            if not os.path.exists(path):
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                continue
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    processed += 1
                    cleaned = clean_record(line)
                    if not cleaned:
                        skipped += 1
                        continue
                    # å¯é€‰è½åœ°åˆ° JSONL
                    if fout:
                        fout.write(json.dumps(cleaned, ensure_ascii=False) + '\n')
                    try:
                        action, _id = upsert_row(session, cleaned)
                        if action == 'inserted':
                            inserted += 1
                        else:
                            updated += 1
                    except Exception as e:
                        skipped += 1
                        print(f"âŒ upsert å¤±è´¥: {e}")
                    if (inserted + updated) % commit_every == 0:
                        session.commit()
        session.commit()

    if fout:
        fout.close()
    return {'inserted': inserted, 'updated': updated, 'skipped': skipped, 'processed': processed}


def import_jsonl_insert_only(path: str, commit_every: int = 500) -> Dict[str, int]:
    """ä»æ¸…æ´—åçš„ JSONL å¯¼å…¥æ•°æ®åº“ï¼ˆä»…æ’å…¥ï¼Œä¸å»é‡ä¹Ÿä¸è¦†ç›–ï¼‰ã€‚è¿”å›ç»Ÿè®¡ã€‚"""
    if not path or not os.path.exists(path):
        raise FileNotFoundError(f'æ–‡ä»¶ä¸å­˜åœ¨: {path}')
    inserted = skipped = processed = 0
    with Session(engine) as session:
        with open(path, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                processed += 1
                line = line.strip()
                if not line:
                    skipped += 1
                    continue
                try:
                    row = json.loads(line)
                except Exception:
                    skipped += 1
                    continue
                try:
                    ts = parse_timestamp(row.get('timestamp'))
                    ca = parse_timestamp(row.get('created_at'))
                    msg = Message(
                        timestamp=ts,
                        title=row.get('title') or '',
                        description=row.get('description') or '',
                        links=row.get('links') or {},
                        tags=row.get('tags') or [],
                        source=row.get('source') or 'cleaned_export',
                        channel=row.get('channel') or '',
                        group_name=row.get('group_name') or '',
                        bot=row.get('bot') or '',
                        created_at=ca,
                    )
                    session.add(msg)
                    inserted += 1
                    if inserted % commit_every == 0:
                        session.commit()
                except Exception as e:
                    skipped += 1
                    print(f"âŒ æ’å…¥å¤±è´¥: {e}")
            session.commit()
    print(f"\n===== JSONL å¯¼å…¥å®Œæˆï¼ˆä»…æ’å…¥ï¼‰ =====")
    print(f"è¯»å–è¡Œæ•°: {processed}")
    print(f"æˆåŠŸæ’å…¥: {inserted} æ¡")
    print(f"è·³è¿‡(è§£æé”™è¯¯ç­‰): {skipped} æ¡")
    return {'processed': processed, 'inserted': inserted, 'skipped': skipped}


def main():
    parser = argparse.ArgumentParser(description='æŠ½æ ·æ¸…æ´— all_channels_export_* æ•°æ®å¹¶å¯é€‰å¯¼å…¥æ ·æœ¬åˆ°æ•°æ®åº“')
    parser.add_argument('--interval', type=int, default=1000, help='æŠ½æ ·é—´éš”ï¼ˆæ¯ N è¡ŒæŠ½ 1 è¡Œï¼‰')
    parser.add_argument('--limit', type=int, default=2000, help='æ ·æœ¬æ•°é‡ä¸Šé™ï¼ˆå¤šæ–‡ä»¶æ€»è®¡ï¼‰')
    parser.add_argument('--import', dest='do_import', action='store_true', help='æ˜¯å¦å¯¼å…¥æ ·æœ¬åˆ°æ•°æ®åº“')
    parser.add_argument('--import_n', type=int, default=20, help='å¯¼å…¥çš„æ ·æœ¬æ¡æ•°ï¼ˆæŒ‰æˆåŠŸè®¡æ•°ï¼‰')
    parser.add_argument('--inputs', nargs='*', default=[
        'all_channels_export_20250906_161213.txt',
        'all_channels_export_20250907_020701.txt'
    ], help='è¾“å…¥æ–‡ä»¶åˆ—è¡¨')
    parser.add_argument('--output', default='cleaned_sample.jsonl', help='æ¸…æ´—æ ·æœ¬è¾“å‡ºè·¯å¾„')
    # æ–°å¢ï¼šå…¨é‡ä¸è¦†ç›–å¯¼å…¥
    parser.add_argument('--full', dest='do_full', action='store_true', help='å…¨é‡æ¸…æ´—ï¼ˆä¸æŠ½æ ·ï¼‰')
    parser.add_argument('--upsert', dest='do_upsert', action='store_true', help='æŒ‰é“¾æ¥å»é‡å¹¶è¦†ç›–å¯¼å…¥ï¼ˆæ•°æ®åº“ï¼‰')
    parser.add_argument('--output_full', default='', help='å…¨é‡æ¸…æ´—JSONLè¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    # æ–°å¢ï¼šä» JSONL å¯¼å…¥ï¼ˆä»…æ’å…¥ï¼‰ä¸æ‰¹é‡æäº¤é—´éš”
    parser.add_argument('--import_json', default='', help='ä»æ¸…æ´—JSONLå¯¼å…¥æ•°æ®åº“ï¼ˆä»…æ’å…¥ï¼Œä¸å»é‡ï¼‰')
    parser.add_argument('--commit_every', type=int, default=500, help='å¯¼å…¥/è¦†ç›–æ—¶çš„æ‰¹é‡æäº¤é—´éš”')

    args = parser.parse_args()

    run_start_naive = to_naive_beijing(datetime.now(BEIJING_TZ))

    # å…¨é‡è¦†ç›–å¯¼å…¥æ¨¡å¼
    if args.do_full and args.do_upsert:
        print(f"ğŸš€ å¼€å§‹å…¨é‡æ¸…æ´—å¹¶è¦†ç›–å¯¼å…¥ï¼šæºæ–‡ä»¶ {len(args.inputs)} ä¸ªï¼ŒæŒ‰é“¾æ¥ç›¸åŒå»é‡ï¼Œæ•°æ®åº“ç›´æ¥è¦†ç›–")
        stats = process_full_upsert(args.inputs, args.output_full)
        print(f"\n===== å…¨é‡è¦†ç›–å¯¼å…¥å®Œæˆ =====")
        print(f"å¤„ç†è¡Œæ•°: {stats['processed']}")
        print(f"æ’å…¥: {stats['inserted']} æ¡")
        print(f"è¦†ç›–æ›´æ–°: {stats['updated']} æ¡")
        print(f"è·³è¿‡(æ— é“¾æ¥/å™ªå£°/é”™è¯¯): {stats['skipped']} æ¡")
        print("ğŸ‰ å¤„ç†å®Œæˆï¼")
        return

    # æ–°å¢ï¼šå…¨é‡æ¸…æ´—ä»…è¾“å‡ºï¼ˆæ¸…æ´—é˜¶æ®µå·²å»é‡ï¼‰
    if args.do_full and not args.do_upsert:
        if not args.output_full:
            print("âŒ å…¨é‡æ¸…æ´—ï¼ˆä»…è¾“å‡ºï¼‰éœ€è¦æŒ‡å®š --output_full è·¯å¾„")
            return
        print(f"ğŸš€ å¼€å§‹å…¨é‡æ¸…æ´—ï¼ˆä»…è¾“å‡ºJSONLï¼Œæ¸…æ´—é˜¶æ®µå»é‡ï¼‰ï¼Œæºæ–‡ä»¶ {len(args.inputs)} ä¸ª")
        stats = process_full_clean_only(args.inputs, args.output_full, dedup=True)
        print("ğŸ‰ å¤„ç†å®Œæˆï¼")
        return

    # æ–°å¢ï¼šä» JSONL å¯¼å…¥æ•°æ®åº“ï¼ˆä»…æ’å…¥ï¼‰
    if args.import_json:
        print(f"ğŸ”„ ä» {args.import_json} å¯¼å…¥æ•°æ®åº“ï¼ˆä»…æ’å…¥ï¼Œä¸å»é‡ï¼‰...")
        stats = import_jsonl_insert_only(args.import_json, commit_every=args.commit_every)
        print("ğŸ‰ å¤„ç†å®Œæˆï¼")
        return

    # æŠ½æ ·æ¸…æ´— + å¯é€‰æ ·æœ¬å¯¼å…¥æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    print(f"ğŸš€ å¼€å§‹æŠ½æ ·æ¸…æ´—ï¼šæ¯ {args.interval} è¡ŒæŠ½ 1 è¡Œï¼Œæœ€å¤š {args.limit} æ¡æ ·æœ¬ï¼ˆå¤šæ–‡ä»¶åˆå¹¶ï¼‰")
    rows = sample_and_clean(args.inputs, args.interval, args.limit)
    if not rows:
        print("âŒ æœªå¾—åˆ°ä»»ä½•æœ‰æ•ˆæ ·æœ¬ï¼ˆå¯èƒ½æ²¡æœ‰ç½‘ç›˜é“¾æ¥æˆ–æ–‡ä»¶ä¸å¯è¯»ï¼‰")
        return

    write_jsonl(args.output, rows)

    if args.do_import:
        print(f"ğŸ”„ æ­£åœ¨å¯¼å…¥å‰ {args.import_n} æ¡æ ·æœ¬åˆ°æ•°æ®åº“ï¼ˆæŒ‰æˆåŠŸè®¡æ•°ï¼‰...")
        inserted_ids = import_first_n(rows, args.import_n)
        # ä¼˜å…ˆæŒ‰ ID ç²¾ç¡®å›æ˜¾
        query_and_print_by_ids(inserted_ids)
        # è‹¥æœªå›æ˜¾åˆ°ä»»ä½•è®°å½•ï¼Œå† fallback æŒ‰æ—¶é—´çª—å£æŸ¥è¯¢
        if not inserted_ids:
            query_and_print_imported(args.import_n, run_start_naive)

    print("ğŸ‰ å¤„ç†å®Œæˆï¼")


if __name__ == '__main__':
    main()